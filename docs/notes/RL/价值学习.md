# Q Learning

Value-based 算法，Q 即为 $Q(s,a)$，指某一时刻的 $s$ 状态下，采取动作 $a$ 能够获得收益的期望。环境会根据 agent 的动作反馈相应的回报 reward 。算法的主要思想是将 State 和 Action 构建成一张 Q-table 来存储 Q 值，根据 Q 值来选取能够获得最大收益的动作。贪婪法。
Q Learning 的主要优势是使用了时间差分法（融合蒙特卡洛和动态规划），可进行离线学习。

TD 算法是一大类算法，常见的有 Q 学习和 SARSA。Q 学习的目的是学到最优的动作价值函数 $Q_{\star}$，而 SARSA 目的是学习动作价值函数 $Q_{\pi}$

## 算法推导

贝尔曼方程蒙特卡洛近似，并将动作价值函数替换为神经网络，得到：

$$ Q_{\star}(s_t,a_t;w) = r_t + \gamma * maxQ_{\star}(s_{t+1}, a) $$

损失函数使用均方误差

$$ L(w) = \frac{1}{2}[Q(s_t,a_t;w) - \hat{y}]^2 $$

使用梯度下降进行优化

## 训练流程

给定四元组 $(s_t,a_t,r_t,s_{t+1})$，可计算出 DQN 预测值

$$ \hat{q_t} = Q(s_t,a_t;w) $$

计算损失函数，并使用梯度下降进行更新参数。

该算法所需数据为四元组，与策略 $\pi$ 无关，所以可以用任何策略控制智能体，同时记录下算法的运动轨迹，作为 DQN 的训练数据。

DQN 的训练分为两个独立的部分：

- 收集训练数据

用任何策略函数 $\pi$ 控制智能体与环境交互，常用的是 $\epsilon$-greedy 策略

$$
f(n)=
\begin{cases}
argmax_aQ(s_t,a;w), & \text{概率} (1 - \epsilon)\\
\text{均匀抽取 A 中的一个动作}, & \text{概率} (\epsilon)
\end{cases}
$$

把轨迹划分为 $n$ 个 $(s_t,a_t,r_t,s_{t+1})$ 四元组，存入数据，这个数组叫做经验回放(Replay Buffer)

- 更新 DQN 参数 $w$

1. 对 DQN 做正向传播，得到 Q 值

$$ \hat{q_j} = Q(s_j,a_j;w_{now}) $$
$$ \hat{q_{j+1}} = maxQ(s_{j+1}, a; w_{now}) $$

2. 计算 TD 目标和 TD 误差

$$ \hat{y_j} = r_j + \gamma * \hat{q_{j+1}} $$
$$ \delta_j = \hat{q_j} - \hat{y_j} $$

3. 梯度下降更新 DQN 参数

$$ w_{new} = w_{now} - \alpha * \delta_j * g_j $$

收集数据、更新 DQN 参数可同时进行，或者异步进行

## 算法流程：  

- 算法输入：迭代轮数 $T$，状态集 $S$，动作集 $A$，步长 $\alpha$，衰减因子 $\gamma$，探索率 $\epsilon$

- 算法输出：所有的状态和动作对应的价值 $Q$

1. 随机初始化所有状态和动作对应的价值 $Q$，对于终止状态的 $Q$ 值初始化为0

2. for i from 1 to T，进行迭代：
   1. 初始化 $S$ 为当前状态序列的第一个状态；
   2. 用 $\epsilon$-贪婪法在当前状态 $S$ 选择出动作$A$;
   3. 在状态 $S$ 执行当前动作 $A$，得到新状态 $S'$ 和奖励 $R$；
   4. 更新价值函数：$Q(s,a) = Q(s,a)+a[r+\gamma max_{a'}Q(s',a')-Q(s,a)]$
   5. $S=S'$
   6. 如果 $S'$ 是终止状态，则当前轮迭代完毕，否则转到b。

- 若状态和动作特别复杂，使得 Q-Learning 和 SARSA 要维护的 Q 表非常大，甚至远远超过内存，这限制了时序差分算法的应用场景。

## 同策略（On-policy）和异策略（Off-policy）

- 行为策略（Behavior Policy）

在智能体与环境交互的过程中，控制智能体与环境交互的策略称为行为策略。行为策略的作用是收集经验，即观测的环境、动作、奖励

- 目标策略（Target Policy）

训练的目的是得到一个策略函数，在结束训练之后，用这个策略函数来控制智能体，这个策略函数就叫做目标策略。

Q 学习算法用任意的行为策略收集 $(s_t,a_t,r_t,s_{t+1})$ 这样的四元组，然后用它们训练目标策略，即 DQN

行为策略和目标策略可以相同，也可以不同。同策略是指用相同的行为策略和目标策略；异策略是指用不同的行为策略和目标策略。DQN 是异策略。

DQN 是异策略，行为策略可以不同于目标策略，可以用任意的行为策略收集经验，比如最常用的行为策略是 $\epsilon$-greedy。让行为策略带有随机性的好处是能探索更多没见过的状态。

异策略的好处是可以用行为策略收集经验，把 $(s_t,a_t,r_t,s_{t+1})$ 这样的四元组记录到一个数组里，在事后反复利用这些经验去更新目标策略。这个数组被称作经验回放数组(Replay Buffer)，这种训练方式被称作经验回放 (Experience Replay)。经验回放只适用于异策略，不适用于同策略，其原因是收集经验时用的行为策略不同于想要训练出的目标策略。

## SARSA 算法

Q Learning 的目的是学习最优动作价值函数 $Q_{\star}$，而 SARSA 的目的是学习动作价值函数 $Q_{\pi}(s,a)$。

策略函数 $\pi$ 控制智能体，被看做“演员”；而 $Q_\pi$ 评价 $\pi$ 的表现，帮助改进 $\pi$，因此 $Q_\pi$ 被看做“评委”。Actor-Critic 通常使用 SARSA 训练“评委” $Q_\pi$

- Q Learning 与 SARSA 的对比：

Q 学习不依赖于 $\pi$，因此 Q 学习属于异策略 (Off-policy)，可以用经验回放。而 SARSA 依赖于 $\pi$，因此 SARSA 属于同策略 (On-policy)，不能用经
验回放。

Q 学习的目标是学到表格 $\widetilde{Q}$，作为最优动作价值函数 $Q_{\star}$ 的近似。因为 $Q_{\star}$ 与 $\pi$ 无关，所以在理想情况下，不论收集经验用的行为策略 $\pi$ 是什么，都不影响 Q 学习得到的 $Q$。因此，Q 学习属于异策略 (Off-policy)，允许行为策略区别于目标策略。Q 学习允许使用经验回放，可以重复利用过时的经验。

SARSA 算法的目标是学到表格 $q$，作为动作价值函数 $Q_{\pi}$ 的近似。$Q_{\pi}$ 与一个策略 $\pi$ 相对应；用不同的策略 $\pi$，对应 $Q_{\pi}$ 就会不同；策略 $\pi$ 越好，$Q_{\pi}$ 的值越大。经验回放数组里的经验 $(s_j , a_j , r_j , s_{j+1})$ 是过时的行为策略 $\pi_{old}$ 收集到的，与当前策略 $\pi_{now}$ 及其对
应的价值 $Q_{\pi_{now}}$ 对应不上。想要学习 $Q_{\pi}$ 的话，必须要用与当前策略 $\pi_{now}$ 收集到的经验，而不能用过时的 $\pi_{old}$ 收集到的经验。这就是为什么 SARSA 不能用经验回放。

### 训练过程

1. 观测到当前状态 $s_t$，根据当前策略做抽样：$a_t = \pi_{now}(|s_t)$

2. 用价值网络计算 $(s_t, a_t)$ 的值：

$$ \hat{q_j} = q(s_t,a_t;w_{now}) $$

3. 智能体执行动作 $a_t$ 后，观测到奖励 $r_t$ 和新的状态 $s_{t+1}$

4. 根据当前策略做抽样：$\widetilde{a}_{t+1}=\pi_{now}(|s_{t+1})$，但该动作只是假想动作，智能体不执行

5. 用价值网络计算 $(s_{t+1}, \widetilde{a}_{t+1})$
 的价值

$$ \hat{q_{t+1}} = q(s_{t+1}, \widetilde{a}_{t+1}; w_{now}) $$

6. 计算 TD 目标和 TD 误差

$$ \hat{y_t} = r_t + \gamma * \hat{q_{t+1}} $$
$$ \delta_t = \hat{q_t} - \hat{y_t} $$

7. 梯度下降更新 DQN 参数

$$ w_{new} = w_{now} - \alpha * \delta_t * g_t $$

### 多步 TD

# Deep Q-Learning

- 在普通的 Q Learning 中，当状态和动作空间是离散，且维数不高时，可使用 Q-Table 存储每个状态动作对的 Q 值。但当状态和动作空间是高维连续时，使用 Q-Table 就较为困难。此时可将 Q-Table 的更新转换为一个函数拟合问题，使用神经网络来代替 Q-Table 产生 Q 值，即为 DQN。

- DQN 有两个大的创新点：

1. Replay Buffer 样本回放缓冲区或者叫做（Experience replay）；

2. Target Network 目标网络；

## Replay Buffer

- 使用 DQN 模型代替 Q-Table 会遇到两个问题

1. 交互得到的序列存在一定的相关性：交互序列中的状态行动存在着一定的相关性，而对于基于最大似然法的机器学习模型来说，我们有一个很重要的假设：训练样本是独立且来自相同分布的，一旦这个假设不成立，模型的效果就会大打折扣。而上面提到的相关性恰好打破了独立同分布的假设，那么学习得到的值函数模型可能存在很大的波动。

2. 交互数据的使用效率：采用梯度下降法进行模型更新时，模型训练往往需要经过多轮迭代才能收敛，每一次迭代，需要使用一定数量的样本计算梯度，如果每次计算的样本在计算一次梯度后就被丢弃，那么我们就需要花费更多的时间与环境交互并收集样本。

- Replay Buffer 包含了收集样本和采样样本两个过程

1. 收集样本，按照时间顺序存入结构中，若 Replay Buffer 存满，则新样本将会将时间上最久远的样本覆盖，数组大小是个需要调的超参数；

2. 采样样本，从缓存中均匀的随机采样一批样本进行学习，用于稳定训练过程。

- 经验回放的优点

1. 打破序列的相关性。随机抽样的四元组是独立的，消除了相关性；

2. 重复利用收集到的经验，而不是用一次就丢弃。可以用更少的样本数量达到同样的表现。

- 经验回放的局限性

并非所有的强化学习方法都允许重复使用过去的经验。经验回放数组里的数据全都是用行为策略 (Behavior Policy) 控制智能体收集到的。在收集经验同时，我们也在不断地改进策略。策略的变化导致收集经验时用的行为策略是过时的策略，不同于当前我们想要更新的策略——即目标策略 (Target Policy)。也就是说，经验回放数组中的经验通常是过时的行为策略收集的，而我们真正想要学的目标策略不同于过时的行为策略。

有些强化学习方法允许行为策略不同于目标策略。这样的强化学习方法叫做异策略(Off-policy)。比如 Q 学习、确定策略梯度 (DPG) 都属于异策略。由于它们允许行为策略不
同于目标策略，因此过时行为策略收集到的经验可以被重复利用。经验回放适用于异策略。

有些强化学习方法要求行为策略与目标策略必须相同。这样的强化学习方法叫做同策略 (On-policy)。比如 SARSA、REINFORCE、A2C 都属于同策略。它们要求经验必须是当前的目标策略收集到的，而不能使用过时的经验。经验回放不适用于同策略。

- 优先经验回放

优先经验回放给每个四元组一个权重，再根据权重做非均匀随机抽样。

意外情况的样本量少，又极其重要，则意外情况样本应有更高的权重。当 DQN 预测 $Q(s_j,a_j;w)$ 严重偏离真实价值 $Q_\star(s_j,a_j)$，则该样本应该有较高的权重。

优先经验回放对数组里的样本做非均匀抽样，TD 误差大的样本被抽样到的概率大。非均匀抽样的概率会影响学习率，抽样概率大的样本学习率会比较小。

## 高估问题

Q 学习产生高估的原因有两个：

- 自举导致偏差的传播；
- 最大化导致 TD 目标高估真实价值。

DQN 的高估往往是非均匀的，导致 DQN 做出的决策是不可靠的。要避免高估，需要：

- 切断自举
- 避免最大化造成高估

## Target Network （Nature DQN）

要切断自举，可使用另一个神经网络计算 TD 目标，称为目标网络（Target Network）。

模型通过当前时刻的回报和下一时刻的价值估计进行更新，这里存在一些隐患，前面提到数据样本差异可能造成一定的波动，由于数据本身存在着不稳定性 每一轮轮迭代都可能产生一些波动，如果按照上面的计算公式，这些波动会立刻反映到下一个迭代的计算中，这样我们就很难得到一个平稳的模型。为了减轻相关问题带来的影响，我们要尽可能地将两个部分解耦。所以引入了 Target Network，而原本的模型被称为 Behavior Network。

1. 在训练开始时，两个模型使用完全相同的参数。

2. 在训练过程中， Behavior Network 负责与环境交互，得到交互样本。

3. 在学习过程中，由 Q-Learning 得到的目标价值由 Target Network 算得到;然后用它和 Behavior Network 的估计值进行比较得出目标值并更新 Behavior Network。

4. 每当训练完成一定轮数的迭代，Behavior Network 模型的参数就会同步给 Target Network，这样就可以进行下一个阶段的学习了。

5. 通过使用 Target Network，计算目标价值的模型在一段时间内将被固定，这样模型可以减轻模型的波动性。

## Double DQN

- 之前的目标 Q 值都时通过贪婪法计算得到，可能导致过度估计，即最终的算法模型有较大偏差；

- 先在当前 Q 网络中找出最大 Q 值对应的动作，再根据这个动作在目标网络里计算目标 Q 值；

- 缓解最大化造成的高估。

| 类型 | 选择（动作） | 求值（TD目标） | 自举偏差 | 最大化高估 |
| ---- | ------------ | -------------- | -------- | ---------- |
| 原始 | 原始 DQN     | 原始 DQN       | 严重     | 严重       |
| 目标 | 目标 DQN     | 目标 DQN       | 不严重   | 严重       |
| 双Q  | 原始 DQN     | 目标 DQN       | 不严重   | 不严重     |

## Prioritized Replay DQN

- 针对经验回放的部分做优化；

- 该网络根据每个样本的 TD 误差绝对值，给定该样本的优先级正比于 TD 误差值。将这个优先级的值存入经验回放池，该优先级的值会影响到它被采样的概率。

## Dueling DQN

- 优化神经网络结构；

- 将 Q 网络分为两个部分
  - 一部分仅与状态 S 有关，与动作 A 无关，称为价值函数部分，V(S, w, a)；
  - 一部分同时与状态 S 和动作 A 有关，称为优势函数，A(S, A, w, b)；
  - 其中 w 是公共部分的网络参数，a 和 b 分别是独有的参数。
