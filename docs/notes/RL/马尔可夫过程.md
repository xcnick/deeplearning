# 马尔可夫过程

## 马尔可夫链

- 如果说一个状态转移序列，满足马尔可夫，就是说下一状态只取决于当前状态，和之前的状态都是不相关的
- 假设有一个状态历史序列：$h_t = {s_1, s_2, s_3,..., s_t}$，若 $S_t$ 是马尔可夫链，则

$$p(s_{t+1} | s_t) = p(s_{t+1} | h_t) \\
p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t)$$

## 马尔可夫奖励过程（MRP）

### Horizon

指的是一个episode里的最大步数。可以理解为单次任务结束用了多少步

### Return

累计折扣收益

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}... + \gamma^{T-t-1} R_T  \gamma \in [0, 1)$$

### Value Function

$V_t(s)$ 是 $G_t$ 的期望，指从当前状态开始，有可能获取多大的价值

$$V_t(s) = \mathbb{E}[G_t|S_t=s]=\mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^{T-t-1}R_T|s_t=s]$$

利用贝尔曼等式，得到 $V=R+\gamma PV$，其中 R 是中间奖励

可利用迭代方式求解：

- 动态规划
- 蒙特卡洛方法
- 时间差分

## 马尔可夫决策过程（MDP）

### 五元组 $(S,A,P,R,\gamma)$

- S 代表一系列状态
- A 代表一系列动作
- P 代表动作的转移模型
- R 是奖励函数
- gamma 是奖励系数
  - 若为0，则表示仅考虑当前奖励
  - 若为1，则表示未来奖励与当前奖励同等重要

### Policy 在 MDP 中的定义

在某一状态 $S_t$ 下应该采取哪一个 action

$$\pi(a|s)=P(a_t=a|s_t=s)$$

给定一个 policy 后，则将 MDP 问题转换为 MRP 问题。有了 policy ，我们能知道状态 s 下，采取动作 a 的概率是多少。

转移函数：

$$P^{\pi}(s'|s)=\sum_{a \in A} \pi(a|s)P(s'|s,a)$$

回报函数：

$$R^\pi(s)=\sum_{a \in A} \pi(a|s)R(s,a)$$

### Value Function

与MRP中类似，在策略 $\pi$ 下的期望

$$V^\pi(s) = \mathbb{E}[G_t|S_t=s]$$

引入一个动作值函数，指该状态下，采取一个特定 action 后的期望收益

$$q^\pi(s,a) = \mathbb{E}[G_t|S_t=s, A_t=a]$$

上面两个函数的关系：

$$V^\pi(s) = \sum_{a \in A} \pi(a|s)q^T(s,a)$$

### Policy Evaluation

已知 policy ，计算 $V^\pi(s)$，也叫 value prediction

### Prediction

给定一个MDP和策略，把 value function 计算出来

### Control

寻找一个最佳策略，输入 MDP ，输出是最佳价值函数 $V^*$ 和最佳策略 $\pi^*$
