# 策略学习

策略学习是指通过求解一个优化问题，学习出最优策略函数或者它的近似（策略网络）。

## 策略网络

策略函数 $\pi$ 是条件概率质量函数：

$$ \pi(a|s) = P(A=a|S=s)$$

每次观测到一个状态，就用策略函数计算出每个动作的概率值，然后做随机抽样，得到动作 $a$。

使用神经网络 $\pi(a|s;\theta)$ 近似策略函数 $\pi(a|s)$，该神经网络被称为策略网络。

策略网络输出层激活函数是 Softmax

## 策略梯度

随机梯度

$$ g(s,a;\theta) = Q_\star(s, a) * \nabla_{\theta}ln\pi(a|s;\theta) $$

是策略梯度 $\nabla_{\theta}J(\theta)$ 的无偏估计。

对于动作价值函数 $Q_\pi(s,a)$ 用两种方式近似：

- REINFORCE，用实际观测的回报 $u$ 近似 $Q_\pi(s,a)$
- Actor-Critic，用神经网络 $q(s,a;w)$ 近似 $Q_\pi(s,a)$

## REINFORCE

对上述随机梯度公式中的 $Q_\pi$ 做蒙特卡洛近似，替换为回报 $u$

### 训练流程

1. 用策略网络 $\theta_{now}$ 控制智能体从头开始玩一局游戏，得到一条轨迹

$$s_1, a_1, r_1,  s_2, a_2, r_2, ... s_n, a_n, r_n$$

2. 计算所有回报

$$u_t = \sum^{n}_{k=t}{\gamma^{k-t} * r_k}$$

3. 用 ${\{s_t, a_t\}}^{n}_{t=1}$ 作为数据，做反向传播计算

$$\nabla_\theta ln\pi(a_t|s_t;\theta_{now})$$

4. 做随机梯度上升更新策略网络参数

$$\theta_{new} = \theta_{now} + \beta * \sum^{n}_{t=1}\gamma^{t-1} * u_t * \nabla_\theta ln\pi(a_t|s_t;\theta_{now})$$

REINFORCE 是一种同策略方法，要求行为策略和目标策略相同，都必须是策略网络 $\pi(a|s;\theta_{now})$，使用当前参数，所以经验回放不适用于 REINFORCE

## Actor-Critic

使用神经网络近似动作价值函数 $Q_\pi(s,a)$，这个神经网络叫做价值网络 $q(s,a;w)$，输出向量维度与动作空间维度相同。

价值网络与 DQN 有相同的结构，但两者意义不同，训练算法不同：

- 价值网络是对动作价值函数 $Q_\pi(s,a)$ 的近似，而 DQN 是对最优动作价值函数 $Q_\star(s,a)$ 的近似
- 对价值网络的训练使用的是 SARSA 算法，属于同策略，不能用经验回放。对 DQN 的训练使用的是 Q 学习算法，属于异策略，可以使用经验回放

### 算法推导

Actor-Critic，演员-评委方法。策略网络 $\pi(a|s;\theta)$ 相当于演员，基于状态 $s$ 做出动作 $a$。价值网络 $q(s,a;\theta)$ 相当于评委，给演员的表现打分，量化在状态 $s$ 的情况下做出动作 $a$ 的好坏程度

![Actor_Critic](assets/actor-critic.png)

为什么不直接把奖励 $R$ 反馈给策略网络（演员），而是用价值网络（评委）这样一个中介。策略学习的目标函数 $J(\theta)$ 是回报 $U$ 的期望，而不是奖励 $R$ 的期望。虽然能观测到当前的奖励 $R$，但它对价值网络是毫无意义的。训练策略网络（演员）需要的是回报 $U$，也就是未来所有奖励的加权和。

- 训练策略网络（演员）

演员利用当前的状态 $s$，自己的动作 $a$，以及评委的打分 $\hat{q}$，计算近似策略梯度，更新自己的参数 $\theta$。通过这种方式，演员的表现越来越受评委的好评，获得的评分 $\hat{q}$ 越来越高

- 训练价值网络（评委）

在上述训练方式中，演员并不是真正的表现更好，而是更迎合评委的喜好。评委的水平也很重要，当评委的打分 $\hat{q}$ 真正反映出动作价值 $Q_\pi$，演员的水平才能真正提高。开始的时候，价值网络的参数 $w$ 是随机的，用 SARSA 算法更新 $w$，提升评委的水平。每次从环境中观测到一个奖励 $r$，就用 $r$ 来校准评委的打分

### 训练流程

1. 观测到当前状态 $s_t$，根据策略网络做决策：$a - \pi(|s_t;\theta_{now})$，并让智能体执行动作 $a_t$

2. 从环境中观测到奖励 $r_t$ 和新的状态 $s_{t+1}$

3. 根据策略网络做出决策 $a_{t+1} - \pi(|s_{t+1};\theta_{now}$，但不让智能体执行动作 $a_{t+1}$

4. 让价值网络打分

$$ \hat{q_t}=q(s_t,a_t;w_{now}) $$
$$ \hat{q}_{t+1}=q(s_{t+1}, a_{t+1};w_{now}) $$

5. 计算 TD 目标和 TD 误差

$$ \hat{y_t} = r_t + \gamma * \hat{q_{t+1}} $$
$$ \delta_t=\hat{q_t}-\hat{y_t}$$

6. 更新价值网络

$$ w_{new}=w_{now}-\alpha*\delta_t*\nabla_w q(s_t,a_t;w_{now})$$

7. 更新策略网络

$$ \theta_{new}=\theta_{now}+\beta*\hat{q_t}*\nabla_\theta ln\pi(a_t|s_t;\theta_{now})$$

也可以使用目标网络改进训练，区别仅在于第 4 步和第 5 步，$\hat{q}_{t+1}$ 使用目标网络计算，以及 TD 目标和误差也使用目标网络计算