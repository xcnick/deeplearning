{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, PartialShape\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "image_fake = np.random.randn(8, 3, 224, 224)\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "print(input_layer.any_name)\n",
    "#model_onnx.reshape({input_layer: PartialShape([8, 224, 224, 3])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    #config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "\n",
    "for _ in range(100):\n",
    "    request.infer({input_layer.any_name: image_fake})\n",
    "\n",
    "print(\n",
    "    'resnet50:',\n",
    "    timeit.timeit(\n",
    "        'request.infer({input_layer.any_name: image_fake})',\n",
    "        number=1000,\n",
    "        globals=globals()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic shape\n",
    "from openvino.runtime import Core, PartialShape\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "model_onnx.reshape([-1, 224, 224, 3])\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "#print(input_layer.get_partial_shape())\n",
    "#model_onnx.reshape({input_layer: PartialShape([8, 224, 224, 3])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    #config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "\n",
    "image_fake = np.random.randn(4, 224, 224, 3)\n",
    "\n",
    "for _ in range(100):\n",
    "    request.infer([image_fake])\n",
    "\n",
    "print(\n",
    "    'resnet50:',\n",
    "    timeit.timeit(\n",
    "        'request.infer({input_layer.any_name: image_fake})',\n",
    "        number=1000,\n",
    "        globals=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core#, PartialShape\n",
    "#import numpy as np\n",
    "#import timeit\n",
    "\n",
    "#image_fake = np.random.randn(8, 224, 224, 3)\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = '/workspace/case-text-classification/inference/model.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "\n",
    "model_onnx.reshape({'input_1': [-1, (1, 32)], \n",
    "                    'input_2': [1, 32]})\n",
    "# from openvino.offline_transformations import serialize\n",
    "\n",
    "# serialize(\n",
    "#     model=model_onnx,\n",
    "#     model_path=\"/workspace/case-text-classification/inference/onnx_model.xml\",\n",
    "#     weights_path=\"/workspace/case-text-classification/inference/onnx_model.bin\"\n",
    "# )\n",
    "\n",
    "#input_layer = next(iter(model_onnx.inputs))\n",
    "#print(input_layer.shape)\n",
    "#model_onnx.reshape({input_layer: PartialShape([8, 224, 224, 3])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    #config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    config={\"PERFORMANCE_HINT\": \"LATENCY\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "ie = Core()\n",
    "model_path = '/workspace/case-text-classification/inference/model.xml'\n",
    "model_onnx = ie.read_model(model=model_path)\n",
    "\n",
    "#input_layer = next(iter(model_onnx.inputs))\n",
    "#print(input_layer.shape)\n",
    "#model_onnx.reshape({input_layer: PartialShape([8, 224, 224, 3])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU')\n",
    "    #config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    #config={\"PERFORMANCE_HINT\": \"LATENCY\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Saved Model\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import tensorflow as tf\n",
    "\n",
    "saved_model_path = 'saved_model'\n",
    "tf_model = tf.saved_model.load(saved_model_path)\n",
    "\n",
    "# ONNXRuntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    './onnx/resnet50.onnx', providers=['CPUExecutionProvider'])\n",
    "\n",
    "# OpenVINO\n",
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "# INT8\n",
    "ir_model_path = 'pot/results/resnet50_DefaultQuantization/2022-04-20_17-33-03/optimized/resnet50.xml'\n",
    "model_ir = ie.read_model(model=ir_model_path)\n",
    "\n",
    "\n",
    "# input\n",
    "import numpy as np\n",
    "\n",
    "dummy_input = np.random.randn(1, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "tf_output = tf_model(tf.convert_to_tensor(dummy_input))\n",
    "ort_output = ort_session.run(None,\n",
    "                             {ort_session.get_inputs()[0].name: dummy_input})\n",
    "request.infer({input_layer.any_name: dummy_input})\n",
    "ov_output = request.get_output_tensor(0).data\n",
    "\n",
    "np.testing.assert_allclose(tf_output, ort_output[0], rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(tf_output, ov_output, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "   \n",
    "warm_up_iters = 100\n",
    "inference_iters = 1000\n",
    "\n",
    "batch_size = [1, 2, 4, 8, 16]\n",
    "for bs in batch_size:\n",
    "    dummy_input = np.random.randn(bs, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "    # tensorflow saved_model\n",
    "    tf_input = tf.convert_to_tensor(dummy_input)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        tf_model(tf_input)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        tf_model(tf_input)\n",
    "    tf_time = time.time() - start_time\n",
    "    print(f'tensorflow: bs {bs}, {tf_time} s')\n",
    "\n",
    "    # onnxruntime \n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input}\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    ort_time = time.time() - start_time\n",
    "    print(f'onnxruntime: bs {bs}, {ort_time} s')\n",
    "\n",
    "    # openvino\n",
    "    model_onnx.reshape([bs, 224, 224, 3])\n",
    "    compiled_model_onnx = ie.compile_model(\n",
    "        model=model_onnx,\n",
    "        device_name='CPU',\n",
    "        config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    infer_queue = AsyncInferQueue(compiled_model_onnx, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino-fp32: bs {bs}, {ov_time} s')\n",
    "\n",
    "    # openvino INT8\n",
    "    model_ir.reshape([bs, 224, 224, 3])\n",
    "    compiled_model_ir = ie.compile_model(\n",
    "        model=model_ir,\n",
    "        device_name='CPU',\n",
    "        config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    infer_queue = AsyncInferQueue(compiled_model_ir, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino-int8: bs {bs}, {ov_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [1, 2, 4, 8, 16]\n",
    "\n",
    "@tf.function\n",
    "def warp_tf_model(model, inputs):\n",
    "    return model(inputs)\n",
    "\n",
    "for bs in batch_size:\n",
    "    # tensorflow saved_model\n",
    "    saved_model_path = 'saved_model'\n",
    "    keras_model = tf.keras.models.load_model(saved_model_path)\n",
    "    tf_input = tf.convert_to_tensor(dummy_input)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        #keras_model(tf_input)\n",
    "        warp_tf_model(keras_model, tf_input)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        #keras_model(tf_input)\n",
    "        warp_tf_model(keras_model, tf_input)\n",
    "    tf_time = time.time() - start_time\n",
    "    print(f'tensorflow keras: bs {bs}, {tf_time} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int8\n",
    "# OpenVINO\n",
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "\n",
    "ie = Core()\n",
    "ir_model_path = 'pot/results/resnet50_DefaultQuantization/2022-04-20_17-33-03/optimized/resnet50.xml'\n",
    "model_ir = ie.read_model(model=ir_model_path)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "   \n",
    "warm_up_iters = 100\n",
    "inference_iters = 1000\n",
    "\n",
    "batch_size = [1, 2, 4, 8, 16]\n",
    "for bs in batch_size:\n",
    "    dummy_input = np.random.randn(bs, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "    # openvino\n",
    "    model_ir.reshape([bs, 224, 224, 3])\n",
    "    input_layer = next(iter(model_ir.inputs))\n",
    "    compiled_model_ir = ie.compile_model(\n",
    "        model=model_ir,\n",
    "        device_name='CPU',\n",
    "        config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "        \n",
    "    infer_queue = AsyncInferQueue(compiled_model_ir, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino: bs {bs}, {ov_time} s')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61839f7db1b217d44ea212fe58a09e6de221471dead234eee5ac733397de118b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
