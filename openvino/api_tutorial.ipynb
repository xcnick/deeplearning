{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "\n",
    "devices = ie.available_devices\n",
    "for device in devices:\n",
    "    device_name = ie.get_metric(device_name=device, metric_name='FULL_DEVICE_NAME')\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 OpenVINO 模型\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 ONNX 模型\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "onnx_model = './onnx/resnet50-v1-7.onnx'\n",
    "net_onnx = ie.read_network(model=onnx_model)\n",
    "exec_net_onnx = ie.load_network(network=net_onnx, device_name='CPU')\n",
    "\n",
    "print(net_onnx.input_info)\n",
    "input_layer = next(iter(net_onnx.input_info))\n",
    "print(input_layer)\n",
    "\n",
    "print(f\"input layout: {net_onnx.input_info[input_layer].layout}\")\n",
    "print(f\"input precision: {net_onnx.input_info[input_layer].precision}\")\n",
    "print(f\"input shape: {net_onnx.input_info[input_layer].tensor_desc.dims}\")\n",
    "\n",
    "output_layer = next(iter(net_onnx.outputs))\n",
    "print(f\"output layout: {net_onnx.outputs[output_layer].layout}\")\n",
    "print(f\"output precision: {net_onnx.outputs[output_layer].precision}\")\n",
    "print(f\"output shape: {net_onnx.outputs[output_layer].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_onnx.serialize('export_onnx_model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "image_fake = np.random.randn(1, 3, 224, 224)\n",
    "for _ in range(100):\n",
    "    exec_net_onnx.infer({input_layer: image_fake})\n",
    "\n",
    "print('resnet50:', timeit.timeit('exec_net_onnx.infer({input_layer: image_fake})',\n",
    "                                  number=1000, globals=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference a image\n",
    "import cv2\n",
    "\n",
    "image_filename = 'demo.jpeg'\n",
    "image = cv2.imread(image_filename)\n",
    "print(image.shape)\n",
    "\n",
    "N, C, H, W = net_onnx.input_info[input_layer].tensor_desc.dims\n",
    "resized_image = cv2.resize(src=image, dsize=(W, H))\n",
    "print(resized_image.shape)\n",
    "input_data = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), axis=0).astype(np.float32)\n",
    "print(input_data.shape)\n",
    "\n",
    "result = exec_net_onnx.infer({input_layer: input_data})\n",
    "np.argmax(result[next(iter(net_onnx.outputs))])\n",
    "#result[output_layer].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenVINO 2.0 API\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device_name=device, name='FULL_DEVICE_NAME')\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name='CPU')\n",
    "\n",
    "print(model_onnx.inputs)\n",
    "print(model_onnx.outputs)\n",
    "\n",
    "print(compiled_model_onnx.inputs)\n",
    "print(compiled_model_onnx.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.offline_transformations import serialize\n",
    "\n",
    "serialize(model=model_onnx, model_path='export_onnx_model.xml', weights_path='export_onnx_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "classification_model_xml = 'export_onnx_model.xml'\n",
    "model = ie.read_model(model=classification_model_xml)\n",
    "input_layer = next(iter(model.inputs))\n",
    "\n",
    "print(f\"input name: {input_layer.any_name}\")\n",
    "print(f\"input precision: {input_layer.element_type}\")\n",
    "print(f\"input shape: {input_layer.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "classification_model_xml = 'export_onnx_model.xml'\n",
    "model = ie.read_model(model=classification_model_xml)\n",
    "\n",
    "output_layer = next(iter(model.outputs))\n",
    "print(f\"output name: {output_layer.any_name}\")\n",
    "print(f\"output precision: {output_layer.element_type}\")\n",
    "print(f\"output shape: {output_layer.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "image_fake = np.random.randn(1, 3, 224, 224)\n",
    "\n",
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "\n",
    "model_onnx.reshape({input_layer: PartialShape([1, 3, 224, 224])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU')\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "\n",
    "for _ in range(100):\n",
    "    request.infer({input_layer.any_name: image_fake})\n",
    "\n",
    "print(\n",
    "    'resnet50:',\n",
    "    timeit.timeit(\n",
    "        'request.infer({input_layer.any_name: image_fake})',\n",
    "        number=1000,\n",
    "        globals=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference a image\n",
    "import cv2\n",
    "\n",
    "image_filename = 'demo.jpeg'\n",
    "image = cv2.imread(image_filename)\n",
    "print(image.shape)\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name='CPU')\n",
    "input_layer = next(iter(compiled_model_onnx.inputs))\n",
    "output_layer = next(iter(compiled_model_onnx.outputs))\n",
    "\n",
    "N, C, H, W = input_layer.shape\n",
    "resized_image = cv2.resize(src=image, dsize=(W, H))\n",
    "print(resized_image.shape)\n",
    "input_data = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), axis=0).astype(np.float32)\n",
    "print(input_data.shape)\n",
    "\n",
    "# compiled_model inference directly\n",
    "result = compiled_model_onnx([input_data])[output_layer]\n",
    "\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "request.infer(inputs={input_layer.any_name: input_data})\n",
    "result = request.get_output_tensor(output_layer.index).data\n",
    "\n",
    "np.argmax(result, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "resnet50: 26.049174329265952\n"
     ]
    }
   ],
   "source": [
    "# inference sync\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "image_fake = np.random.randn(8, 3, 224, 224)\n",
    "\n",
    "from openvino.runtime import Core, PartialShape\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "\n",
    "model_onnx.reshape({input_layer: PartialShape([8, 3, 224, 224])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "print(compiled_model_onnx.get_property(\"OPTIMAL_NUMBER_OF_INFER_REQUESTS\"))\n",
    "# compiled_model_onnx = ie.compile_model(\n",
    "#     model=model_onnx,\n",
    "#     device_name='CPU',\n",
    "#     config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "# print(compiled_model_onnx.get_property(\"OPTIMAL_NUMBER_OF_INFER_REQUESTS\"))\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "\n",
    "for _ in range(100):\n",
    "    request.infer({input_layer.any_name: image_fake})\n",
    "\n",
    "print(\n",
    "    'resnet50:',\n",
    "    timeit.timeit(\n",
    "        'request.infer({input_layer.any_name: image_fake})',\n",
    "        number=1000,\n",
    "        globals=globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "23.2129807472229\n"
     ]
    }
   ],
   "source": [
    "# inference async\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "from openvino.runtime import Core, AsyncInferQueue, PartialShape\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/resnet50-v1-7.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "\n",
    "model_onnx.reshape({input_layer: PartialShape([8, 3, 224, 224])})\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"LATENCY\", })\n",
    "            #\"PERFORMANCE_HINT_NUM_REQUESTS\": \"64\",\n",
    "            #\"INFERENCE_NUM_THREADS\": \"64\"})\n",
    "print(compiled_model_onnx.get_property(\"OPTIMAL_NUMBER_OF_INFER_REQUESTS\"))\n",
    "# compiled_model_onnx = ie.compile_model(\n",
    "#     model=model_onnx,\n",
    "#     device_name='CPU',\n",
    "#     config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "# print(compiled_model_onnx.get_property(\"OPTIMAL_NUMBER_OF_INFER_REQUESTS\"))\n",
    "#request = compiled_model_onnx.create_infer_request()\n",
    "\n",
    "batch = 8\n",
    "infer_queue = AsyncInferQueue(compiled_model_onnx, batch)\n",
    "image_fake = np.random.randn(batch, 3, 224, 224)\n",
    "\n",
    "for _ in range(100):\n",
    "    #request.infer({input_layer.any_name: image_fake})\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: image_fake})\n",
    "\n",
    "infer_queue.wait_all()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    #request.infer({input_layer.any_name: image_fake})\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: image_fake})\n",
    "infer_queue.wait_all()\n",
    "\n",
    "elapse_time = time.time() - start_time\n",
    "print(elapse_time)\n",
    "# print(\n",
    "#     'resnet50:',\n",
    "#     timeit.timeit(\n",
    "#         'request.infer({input_layer.any_name: image_fake})',\n",
    "#         number=1000,\n",
    "#         globals=globals()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61839f7db1b217d44ea212fe58a09e6de221471dead234eee5ac733397de118b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
