{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES = (\"person\", \"bicycle\", \"car\", \"motorbike \", \"aeroplane \", \"bus \",\n",
    "           \"train\", \"truck \", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "           \"stop sign \", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog \",\n",
    "           \"horse \", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra \", \"giraffe\",\n",
    "           \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "           \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "           \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "           \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife \", \"spoon\", \"bowl\",\n",
    "           \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\",\n",
    "           \"hot dog\", \"pizza \", \"donut\", \"cake\", \"chair\", \"sofa\",\n",
    "           \"pottedplant\", \"bed\", \"diningtable\", \"toilet \", \"tvmonitor\",\n",
    "           \"laptop\t\", \"mouse\t\", \"remote \", \"keyboard \", \"cell phone\",\n",
    "           \"microwave \", \"oven \", \"toaster\", \"sink\", \"refrigerator \", \"book\",\n",
    "           \"clock\", \"vase\", \"scissors \", \"teddy bear \", \"hair drier\",\n",
    "           \"toothbrush \")\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    # y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "    \n",
    "def non_max_suppression(prediction, conf_thres=0.25, nmsThreshold=0.5, agnostic=False):\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "\n",
    "    output = [np.zeros((0, 6))] * prediction.shape[0]\n",
    "\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        conf = np.max(x[:, 5:], axis=1)\n",
    "        j = np.argmax(x[:, 5:], axis=1)\n",
    "        #è½¬ä¸ºarrayï¼š  x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "        re = np.array(conf.reshape(-1) > conf_thres)\n",
    "        #è½¬ä¸ºç»´åº¦\n",
    "        conf = conf.reshape(-1, 1)\n",
    "        j = j.reshape(-1, 1)\n",
    "        #numpyçš„æ‹¼æŽ¥\n",
    "        x = np.concatenate((box, conf, j), axis=1)[re]\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(\n",
    "                descending=True)[:max_nms]]  # sort by confidence\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:,\n",
    "                                        4]  # boxes (offset by class), scores\n",
    "        #è½¬ä¸ºlist ä½¿ç”¨opencvè‡ªå¸¦nms\n",
    "        boxes = boxes.tolist()\n",
    "        scores = scores.tolist()\n",
    "        i = cv2.dnn.NMSBoxes(boxes, scores, conf_thres,\n",
    "                             nmsThreshold)\n",
    "        #i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        output[xi] = x[i]\n",
    "    return output\n",
    "\n",
    "# input\n",
    "srcimg = cv2.imread('./bus.jpg')\n",
    "img = srcimg[..., ::-1]\n",
    "h, w, c = img.shape\n",
    "target = 640\n",
    "\n",
    "# Scale ratio (new / old)\n",
    "scale = min(target / h, target / w)\n",
    "# if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "#     r = min(r, 1.0)\n",
    "# Compute padding\n",
    "new_unpad = int(round(w * scale)), int(round(h * scale))\n",
    "dw, dh = target - new_unpad[0], target - new_unpad[1]  # wh padding\n",
    "dw //= 2  # divide padding into 2 sides\n",
    "dh //= 2\n",
    "\n",
    "img = cv2.resize(img, new_unpad)\n",
    "\n",
    "top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "\n",
    "img = cv2.copyMakeBorder(\n",
    "    img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "    value=(114, 114, 114))  # add border\n",
    "\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "img = np.float32(img)\n",
    "img /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "img = np.transpose(img, (0, 3, 1, 2))\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "# ONNXRuntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    './onnx/yolov5s_st.onnx', providers=['CPUExecutionProvider'])\n",
    "\n",
    "ort_output = ort_session.run(None, {ort_session.get_inputs()[0].name: img})\n",
    "\n",
    "confThreshold = 0.3\n",
    "nmsThreshold = 0.5\n",
    "pred = non_max_suppression(ort_output[0], confThreshold, nmsThreshold, agnostic=False)\n",
    "#draw box\n",
    "for i in pred[0]:\n",
    "    left = int((i[0] - dw) / scale)\n",
    "    top = int((i[1] - dh) / scale)\n",
    "    width = int((i[2] - dw) / scale)\n",
    "    height = int((i[3] - dh) / scale)\n",
    "    conf = i[4]\n",
    "    classId = i[5]\n",
    "    #frame = self.drawPred(frame, classIds[i], confidences[i], left, top, left + width, top + height)\n",
    "    cv2.rectangle(\n",
    "        srcimg, (int(left), int(top)), (int(width), int(height)), (0, 0, 255),\n",
    "        thickness=2)\n",
    "    label = '%.2f' % conf\n",
    "    label = '%s:%s' % (CLASSES[int(classId)], label)\n",
    "    # Display the label at the top of the bounding box\n",
    "    labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                                          1)\n",
    "    top = max(top, labelSize[1])\n",
    "    #cv2.rectangle(srcimg, (int(left), int(top - round(1.5 * labelSize[1]))), (int(left + round(1.5 * labelSize[0])), int(top + baseLine)), (255,255,255), cv2.FILLED)\n",
    "    cv2.putText(\n",
    "        srcimg,\n",
    "        label, (int(left - 20), int(top - 10)),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1, (255, 255, 0),\n",
    "        thickness=2)\n",
    "cv2.imwrite('result.jpg', srcimg)\n",
    "#cv2.imshow('result', srcimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcnick/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "YOLOv5 ðŸš€ v6.1-143-g6ea81bb torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "torch_model = torch.hub.load('/workspace/playground/yolov5', 'yolov5s', source='local', device='cpu')\n",
    "\n",
    "# ONNXRuntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    './onnx/yolov5s.onnx', providers=['CPUExecutionProvider'])\n",
    "\n",
    "# OpenVINO\n",
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/yolov5s.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "# INT8\n",
    "# ir_model_path = 'pot/results/yolov5_DefaultQuantization/2022-04-22_23-44-05/optimized/yolov5.xml'\n",
    "# model_ir = ie.read_model(model=ir_model_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_output = torch_model(torch.tensor(img, dtype=torch.float32))\n",
    "ort_output = ort_session.run(None, {ort_session.get_inputs()[0].name: img})\n",
    "request.infer({input_layer.any_name: img})\n",
    "ov_output = request.get_output_tensor(0).data\n",
    "\n",
    "np.testing.assert_allclose(torch_output, ort_output[0], rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(torch_output, ov_output, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: batch_size 1, 39.41 s\n",
      "onnxruntime: bs 1, 30.29 s\n",
      "openvino-fp32: bs 1, 9.83 s\n",
      "pytorch: batch_size 2, 54.28 s\n",
      "onnxruntime: bs 2, 51.32 s\n",
      "openvino-fp32: bs 2, 19.61 s\n",
      "pytorch: batch_size 4, 84.52 s\n",
      "onnxruntime: bs 4, 99.93 s\n",
      "openvino-fp32: bs 4, 37.99 s\n",
      "pytorch: batch_size 8, 175.21 s\n",
      "onnxruntime: bs 8, 198.82 s\n",
      "openvino-fp32: bs 8, 76.57 s\n",
      "pytorch: batch_size 16, 786.80 s\n",
      "onnxruntime: bs 16, 530.72 s\n",
      "openvino-fp32: bs 16, 157.80 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "   \n",
    "warm_up_iters = 100\n",
    "inference_iters = 1000\n",
    "\n",
    "batch_size = [1, 2, 4, 8, 16]\n",
    "for bs in batch_size:\n",
    "    #dummy_input = np.random.randn(bs, 224, 224, 3).astype(np.float32)\n",
    "    input = np.concatenate([img] * bs, axis=0)\n",
    "\n",
    "    # PyTorch\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warm_up_iters):\n",
    "            torch_output = torch_model(torch.tensor(input))\n",
    "        # inference test\n",
    "        start_time = time.time()\n",
    "        for _ in range(inference_iters):\n",
    "            torch_model(torch.tensor(input))\n",
    "        torch_time = time.time() - start_time\n",
    "        print(f'pytorch: batch_size {bs}, {torch_time:.2f} s')\n",
    "\n",
    "    # onnxruntime \n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input}\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    ort_time = time.time() - start_time\n",
    "    print(f'onnxruntime: bs {bs}, {ort_time:.2f} s')\n",
    "\n",
    "    # openvino\n",
    "    infer_queue = AsyncInferQueue(compiled_model_onnx, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino-fp32: bs {bs}, {ov_time:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.1-143-g6ea81bb torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "torch_model = torch.hub.load('/workspace/playground/yolov5', 'yolov5s', source='local', device='cpu')\n",
    "\n",
    "# ONNXRuntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    './onnx/yolov5s_st.onnx', providers=['CPUExecutionProvider'])\n",
    "\n",
    "# OpenVINO\n",
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/yolov5s_st.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "# INT8\n",
    "ir_model_path = 'pot/results/yolov5_DefaultQuantization/2022-04-22_23-44-05/optimized/yolov5.xml'\n",
    "model_ir = ie.read_model(model=ir_model_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch_output = torch_model(torch.tensor(img, dtype=torch.float32))\n",
    "ort_output = ort_session.run(None, {ort_session.get_inputs()[0].name: img})\n",
    "request.infer({input_layer.any_name: img})\n",
    "ov_output = request.get_output_tensor(0).data\n",
    "\n",
    "np.testing.assert_allclose(torch_output, ort_output[0], rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(torch_output, ov_output, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch: 39.47 s\n",
      "onnxruntime: 29.76 s\n",
      "openvino-fp32: 9.78 s\n",
      "openvino-int8: 3.713121175765991 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "   \n",
    "warm_up_iters = 100\n",
    "inference_iters = 1000\n",
    "\n",
    "# PyTorch\n",
    "with torch.no_grad():\n",
    "    for _ in range(warm_up_iters):\n",
    "        torch_output = torch_model(torch.tensor(img))\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        torch_model(torch.tensor(img))\n",
    "    torch_time = time.time() - start_time\n",
    "    print(f'pytorch: {torch_time:.2f} s')\n",
    "\n",
    "# onnxruntime \n",
    "ort_inputs = {ort_session.get_inputs()[0].name: img}\n",
    "# warm up\n",
    "for _ in range(warm_up_iters):\n",
    "    ort_session.run(None, ort_inputs)\n",
    "# inference test\n",
    "start_time = time.time()\n",
    "for _ in range(inference_iters):\n",
    "    ort_session.run(None, ort_inputs)\n",
    "ort_time = time.time() - start_time\n",
    "print(f'onnxruntime: {ort_time:.2f} s')\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model_onnx, 16)\n",
    "# warm up\n",
    "for _ in range(warm_up_iters):\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: img})\n",
    "infer_queue.wait_all()\n",
    "# inference test\n",
    "start_time = time.time()\n",
    "for _ in range(inference_iters):\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: img})\n",
    "infer_queue.wait_all()\n",
    "ov_time = time.time() - start_time\n",
    "print(f'openvino-fp32: {ov_time:.2f} s')\n",
    "\n",
    "# # openvino INT8\n",
    "compiled_model_ir = ie.compile_model(\n",
    "    model=model_ir,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "infer_queue = AsyncInferQueue(compiled_model_ir, 16)\n",
    "# warm up\n",
    "for _ in range(warm_up_iters):\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: img})\n",
    "infer_queue.wait_all()\n",
    "# inference test\n",
    "start_time = time.time()\n",
    "for _ in range(inference_iters):\n",
    "    infer_queue.start_async(inputs={input_layer.any_name: img})\n",
    "infer_queue.wait_all()\n",
    "ov_time = time.time() - start_time\n",
    "print(f'openvino-int8: {ov_time:.2f} s')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61839f7db1b217d44ea212fe58a09e6de221471dead234eee5ac733397de118b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
