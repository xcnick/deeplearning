{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 17:54:41.335635: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-05-17 17:54:41.335722: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: b7eacc96c929\n",
      "2022-05-17 17:54:41.335738: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: b7eacc96c929\n",
      "2022-05-17 17:54:41.335914: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.47.3\n",
      "2022-05-17 17:54:41.335957: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.47.3\n",
      "2022-05-17 17:54:41.335967: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.47.3\n",
      "2022-05-17 17:54:41.336292: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow Saved Model\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import tensorflow as tf\n",
    "\n",
    "saved_model_path = 'mobilenetv3_saved_model'\n",
    "tf_model = tf.saved_model.load(saved_model_path)\n",
    "\n",
    "# ONNXRuntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_session = ort.InferenceSession(\n",
    "    './onnx/mobilenetv3.onnx', providers=['CPUExecutionProvider'])\n",
    "\n",
    "# OpenVINO\n",
    "from openvino.runtime import Core, AsyncInferQueue\n",
    "\n",
    "ie = Core()\n",
    "onnx_model_path = './onnx/mobilenetv3.onnx'\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "input_layer = next(iter(model_onnx.inputs))\n",
    "compiled_model_onnx = ie.compile_model(\n",
    "    model=model_onnx,\n",
    "    device_name='CPU',\n",
    "    config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "request = compiled_model_onnx.create_infer_request()\n",
    "# INT8\n",
    "ir_model_path = 'pot/results/mobilenetv3_DefaultQuantization/2022-05-16_22-03-46/optimized/mobilenetv3.xml'\n",
    "model_ir = ie.read_model(model=ir_model_path)\n",
    "\n",
    "\n",
    "# input\n",
    "import numpy as np\n",
    "\n",
    "dummy_input = np.random.randn(1, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "tf_output = tf_model(tf.convert_to_tensor(dummy_input))\n",
    "ort_output = ort_session.run(None,\n",
    "                             {ort_session.get_inputs()[0].name: dummy_input})\n",
    "request.infer({input_layer.any_name: dummy_input})\n",
    "ov_output = request.get_output_tensor(0).data\n",
    "\n",
    "np.testing.assert_allclose(tf_output, ort_output[0], rtol=1e-03, atol=1e-05)\n",
    "np.testing.assert_allclose(tf_output, ov_output, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: bs 1, 13.19 s\n",
      "onnxruntime: bs 1, 5.72 s\n",
      "openvino-fp32: bs 1, 0.30 s\n",
      "openvino-int8: bs 1, 0.24 s\n",
      "tensorflow: bs 2, 19.56 s\n",
      "onnxruntime: bs 2, 10.22 s\n",
      "openvino-fp32: bs 2, 0.53 s\n",
      "openvino-int8: bs 2, 0.42 s\n",
      "tensorflow: bs 4, 27.05 s\n",
      "onnxruntime: bs 4, 16.93 s\n",
      "openvino-fp32: bs 4, 0.92 s\n",
      "openvino-int8: bs 4, 0.76 s\n",
      "tensorflow: bs 8, 34.33 s\n",
      "onnxruntime: bs 8, 30.07 s\n",
      "openvino-fp32: bs 8, 1.99 s\n",
      "openvino-int8: bs 8, 1.38 s\n",
      "tensorflow: bs 16, 44.04 s\n",
      "onnxruntime: bs 16, 52.53 s\n",
      "openvino-fp32: bs 16, 4.04 s\n",
      "openvino-int8: bs 16, 2.94 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "   \n",
    "warm_up_iters = 100\n",
    "inference_iters = 1000\n",
    "\n",
    "batch_size = [1, 2, 4, 8, 16]\n",
    "for bs in batch_size:\n",
    "    dummy_input = np.random.randn(bs, 224, 224, 3).astype(np.float32)\n",
    "\n",
    "    # tensorflow saved_model\n",
    "    tf_input = tf.convert_to_tensor(dummy_input)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        tf_model(tf_input)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        tf_model(tf_input)\n",
    "    tf_time = time.time() - start_time\n",
    "    print(f'tensorflow: bs {bs}, {tf_time:.2f} s')\n",
    "\n",
    "    # onnxruntime \n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input}\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        ort_session.run(None, ort_inputs)\n",
    "    ort_time = time.time() - start_time\n",
    "    print(f'onnxruntime: bs {bs}, {ort_time:.2f} s')\n",
    "\n",
    "    # openvino\n",
    "    model_onnx.reshape([bs, 224, 224, 3])\n",
    "    compiled_model_onnx = ie.compile_model(\n",
    "        model=model_onnx,\n",
    "        device_name='CPU',\n",
    "        config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    infer_queue = AsyncInferQueue(compiled_model_onnx, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino-fp32: bs {bs}, {ov_time:.2f} s')\n",
    "\n",
    "    # openvino INT8\n",
    "    model_ir.reshape([bs, 224, 224, 3])\n",
    "    compiled_model_ir = ie.compile_model(\n",
    "        model=model_ir,\n",
    "        device_name='CPU',\n",
    "        config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "    infer_queue = AsyncInferQueue(compiled_model_ir, 16)\n",
    "    # warm up\n",
    "    for _ in range(warm_up_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    # inference test\n",
    "    start_time = time.time()\n",
    "    for _ in range(inference_iters):\n",
    "        infer_queue.start_async(inputs={input_layer.any_name: dummy_input})\n",
    "    infer_queue.wait_all()\n",
    "    ov_time = time.time() - start_time\n",
    "    print(f'openvino-int8: bs {bs}, {ov_time:.2f} s')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61839f7db1b217d44ea212fe58a09e6de221471dead234eee5ac733397de118b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
